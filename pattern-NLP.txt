import numpy
from tensorflow import keras
max_review_length = 1600
top_words = 10000


# Using keras to load the dataset with the top_words
def import_dataset():
    (X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data(num_words=top_words)
    return X_train, X_test, y_train, y_test


# Pad the sequences to the same length
def preprocess_data(X_train, X_test):
    new_X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_review_length)
    new_X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length)
    return new_X_train, new_X_test


def create_model():
    # Using embedding from Keras
    embedding_vecor_length = 300
    model = keras.models.Sequential()
    model.add(keras.layers.Embedding(top_words, embedding_vecor_length, input_length=max_review_length))

    model.add(keras.layers.Convolution1D(64, 3, padding='same'))
    model.add(keras.layers.Convolution1D(32, 3, padding='same'))
    model.add(keras.layers.Convolution1D(16, 3, padding='same'))
    model.add(keras.layers.Flatten())
    model.add(keras.layers.Dropout(0.2))
    model.add(keras.layers.Dense(180,activation='relu'))
    model.add(keras.layers.Dropout(0.2))
    model.add(keras.layers.Dense(1,activation='sigmoid'))

    model.summary()
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model


def train_model(model, X_train, y_train):
    # Log to Tensorboard
    tensorBoardCallback = keras.callbacks.TensorBoard(log_dir='./logs', write_graph=True)
    model.fit(X_train, y_train, epochs=3, callbacks=[tensorBoardCallback], batch_size=64)


def evaluate_model(model, X_test, y_test):
    # Evaluation on the test set
    scores = model.evaluate(X_test, y_test, verbose=1)
    print("Accuracy: %.2f%%" % (scores[1]*100))


def run():
    X_train, X_test, y_train, y_test = import_dataset()
    X_train, X_test = preprocess_data(X_train, X_test)
    model = create_model()
    train_model(model, X_train, y_train)
    evaluate_model(model, X_test, y_test)


run()
from socialsent import embedding_transformer
from scipy.sparse import csr_matrix
from multiprocessing import Pool
from sklearn.linear_model import LogisticRegression, Ridge
"from socialsent import util
import functools
import numpy as np

from socialsent.graph_construction import similarity_matrix, transition_matrix

"""
A set of methods for inducing polarity lexicons using word embeddings and seed words.
"""

def pmi(count_embeds, positive_seeds, negative_seeds, smooth=0.01, **kwargs):
   
    w_index = count_embeds.wi
    c_index = count_embeds.ci
    counts = count_embeds.m
    polarities = {}
    for w in count_embeds.iw:
        if w not in positive_seeds and w not in negative_seeds:
            pol = sum(np.log(counts[w_index[w], c_index[seed]] + smooth) 
                    - np.log(counts[w_index[seed],:].sum()) for seed in positive_seeds)
            pol -= sum(np.log(counts[w_index[w], c_index[seed]] + smooth) 
                    - np.log(counts[w_index[seed],:].sum())for seed in negative_seeds)
            polarities[w] = pol
    return polarities
def dist(embeds, positive_seeds, negative_seeds, **kwargs):
    polarities = {}
    sim_mat = similarity_matrix(embeds, **kwargs)
    for i, w in enumerate(embeds.iw):
        if w not in positive_seeds and w not in negative_seeds:
            pol = sum(sim_mat[embeds.wi[p_seed], i] for p_seed in positive_seeds)
            pol -= sum(sim_mat[embeds.wi[n_seed], i] for n_seed in negative_seeds)
            polarities[w] = pol
    return polarities

def densify(embeddings, positive_seeds, negative_seeds, 
        transform_method=embedding_transformer.apply_embedding_transformation, **kwargs):
    p_seeds = {word:1.0 for word in positive_seeds}
    n_seeds = {word:1.0 for word in negative_seeds}
    new_embeddings = embeddings
    new_embeddings = embedding_transformer.apply_embedding_transformation(
            embeddings, p_seeds, n_seeds, n_dim=1,  **kwargs)
    polarities = {w:new_embeddings[w][0] for w in embeddings.iw}
    return polarities


def random_walk(embeddings, positive_seeds, negative_seeds, beta=0.9, **kwargs):
      def run_random_walk(M, teleport, beta, **kwargs):
      def update_seeds(r):
            r += (1 - beta) * teleport / np.sum(teleport)
        return run_iterative(M * beta, np.ones(M.shape[1]) / M.shape[1], update_seeds, **kwargs)

    if not type(positive_seeds) is dict:
        positive_seeds = {word:1.0 for word in positive_seeds}
        negative_seeds = {word:1.0 for word in negative_seeds}
    words = embeddings.iw
    M = transition_matrix(embeddings, **kwargs)
    rpos = run_random_walk(M, weighted_teleport_set(words, positive_seeds), beta, **kwargs)
    rneg = run_random_walk(M, weighted_teleport_set(words, negative_seeds), beta, **kwargs)
    return {w: rpos[i] / (rpos[i] + rneg[i]) for i, w in enumerate(words)}


def label_propagate_probabilistic(embeddings, positive_seeds, negative_seeds, **kwargs):
    words = embeddings.iw
    M = transition_matrix(embeddings, **kwargs)
    pos, neg = teleport_set(words, positive_seeds), teleport_set(words, negative_seeds)
    def update_seeds(r):
        r[pos] = [1, 0]
        r[neg] = [0, 1]
        r /= np.sum(r, axis=1)[:, np.newaxis]
    r = run_iterative(M, np.random.random((M.shape[0], 2)), update_seeds, **kwargs)
    return {w: r[i][0] / (r[i][0] + r[i][1]) for i, w in enumerate(words)}


def label_propagate_continuous(embeddings, positive_seeds, negative_seeds, **kwargs):
    words = embeddings.iw
    M = transition_matrix(embeddings, **kwargs)
    pos, neg = teleport_set(words, positive_seeds), teleport_set(words, negative_seeds)
    def update_seeds(r):
        r[pos] = 1
        r[neg] = -1
    r = run_iterative(M, np.zeros(M.shape[0]), update_seeds, **kwargs)
    return {w: r[i] for i, w in enumerate(words)}


def graph_propagate(embeddings, positive_seeds, negative_seeds, **kwargs):
    
    def run_graph_propagate(seeds, alpha_mat, trans_mat, T=1, **kwargs):
        def get_rel_edges(ind_set):
            rel_edges = set([])
            for node in ind_set:
                rel_edges = rel_edges.union(
                        [(node, other) for other in trans_mat[node,:].nonzero()[1]])
            return rel_edges

        for seed in seeds:
            F = set([seed])
            for t in range(T):
                for edge in get_rel_edges(F):
                    alpha_mat[seed, edge[1]] = max(
                            alpha_mat[seed, edge[1]], 
                            alpha_mat[seed, edge[0]] * trans_mat[edge[0], edge[1]])
                    F.add(edge[1])
        return alpha_mat

    M = similarity_matrix(embeddings, **kwargs)
    M = (M + M.T)/2
    print "Getting positive scores.."
    pos_alpha = M.copy()
    neg_alpha = M.copy()
    M = csr_matrix(M)
    pos_alpha = run_graph_propagate([embeddings.wi[seed] for seed in positive_seeds],
            pos_alpha, M, **kwargs)
    pos_alpha = pos_alpha + pos_alpha.T
    print "Getting negative scores.."
    neg_alpha = run_graph_propagate([embeddings.wi[seed] for seed in negative_seeds],
            neg_alpha, M, **kwargs)
    neg_alpha = neg_alpha + neg_alpha.T
    print "Computing final scores..."
    polarities = {}
    index = embeddings.wi
    pos_pols = {w:1.0 for w in positive_seeds}
    for w in negative_seeds:
        pos_pols[w] = 0.0
    neg_pols = {w:1.0 for w in negative_seeds}
    for w in positive_seeds:
        neg_pols[w] = 0.0
    for w in util.logged_loop(index):
        if w not in positive_seeds and w not in negative_seeds:
            pos_pols[w] = sum(pos_alpha[index[w], index[seed]] for seed in positive_seeds if seed in index) 
            neg_pols[w] = sum(neg_alpha[index[w], index[seed]] for seed in negative_seeds if seed in index)
    beta = np.sum(pos_pols.values()) / np.sum(neg_pols.values())
    for w in index:
        polarities[w] = pos_pols[w] - beta * neg_pols[w]
    return polarities


### HELPER METHODS #####

def teleport_set(words, seeds):
    return [i for i, w in enumerate(words) if w in seeds]

def weighted_teleport_set(words, seed_weights):
    return np.array([seed_weights[word] if word in seed_weights else 0.0 for word in words])

def run_iterative(M, r, update_seeds, max_iter=50, epsilon=1e-6, **kwargs):
    for i in range(max_iter):
        last_r = np.array(r)
        r = np.dot(M, r)
        update_seeds(r)
        if np.abs(r - last_r).sum() < epsilon:
            break
    return r

